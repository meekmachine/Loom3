# Latticework Roadmap

Latticework is the set of agencies inside `src/latticework/` that transform microphone and webcam signals into verbal and nonverbal behaviors for LoomLarge avatars. Each folder equals one agency. LoomLarge itself just loads the scene, exposes host capabilities (blendshape/bone control, audio playback), and lets these agencies run; it does not have its own animation logic.

## Architectural Facts

- **Agency Encapsulation**: Every behavior lives in a single agency. Examples: `transcription/` (mic → words), `conversation/` (dialog orchestration), `tts/` (text → speech + phonemes), `lipsync/` (phonemes → visemes), `prosodic/` (speech rhythm gestures), `eyeHeadTracking/` (webcam/mouse → gaze), `blink/` (autonomous blinks), `animation/` (applies curves/snippets to the host), `hair/` (specialized physics tweaks). Legacy versions live in `old_agencies/`.
- **Per-Agent Instancing**: Every agent/persona gets its own set of agencies. More humans simply introduce more mic/webcam streams; more avatars spin up parallel agency bundles. No shared singletons.
- **Sensor Ownership**: Agencies capture raw mic + webcam data themselves. Today this is browser APIs; soon we layer LiveKit so the same data flows through remote streams.
- **Animation Agency**: All motion on the avatar flows through the `animation` agency. It accepts curves generated by other agencies or pre-authored snippets and can retime/re-scale them (speed, intensity, playback direction) before applying them via the host engine bindings.
- **Outputs**: Agencies emit two classes of cues:
  - **Verbal**: audio buffers, phoneme timelines, transcribed text.
  - **Nonverbal**: animation curves/snippets for face/head/body, symbolic gestures (e.g., “raise brow”) that the animation agency turns into curves.

## Workstream 1 – Ground Truth + Completion

1. **Agency Registry**
   - Enumerate every folder under `src/latticework/`, noting its inputs, outputs, and whether it already follows the Service → Machine → Scheduler pattern.
   - Link to the relevant README inside each folder so future changes reference real files.
2. **Complete Trinity Pattern**
   - `transcription/`: add `transcriptionMachine` + scheduler so mic → text flow is deterministic regardless of browser or LiveKit source.
   - `tts/`: follow the architecture plan in `src/latticework/tts/TTS_AGENCY_ARCHITECTURE.md`; publish public API docs.
   - Verify `lipsync/`, `eyeHeadTracking/`, `prosodic/`, and `animation/` documents clearly state how they depend on host capabilities.
3. **Per-Agent Factory**
   - Build `createAgentLatticework(hostCaps, micSource, camSource)` that instantiates every agency for one avatar and connects sensor streams.
   - Add tests to prove multiple agents + multiple humans can run simultaneously without shared state collisions.
4. **Documentation Drafts**
   - README “chapters” for Project Setup (how LoomLarge exposes host capabilities) and Agency Pattern (how each folder works). Include file references so we can verify every statement.

## Workstream 2 – Signals, Persona Tuning, and LiveKit

1. **Signal Contracts**
   - Define structured messages (JSON schemas) for agent-to-agent influence (interrupts, empathy cues). Messages travel via LoomLarge events or LiveKit data channels. Agencies never mutate each other directly.
2. **Persona Weighting**
   - Extend persona configs so traits (e.g., from Cybernetic Big Five) drive agency parameters—how aggressively `prosodic` pulses, how `conversation` chooses strategies, etc.
   - Provide visualization tools to preview the effect of trait changes.
3. **LiveKit Sensor Support**
   - Wrap microphone/webcam acquisition so the same API can source local streams or LiveKit remote tracks. Document how each agency selects the source.
4. **Rapport & Strategic Agencies**
   - Promote the rapport and strategic interaction logic inside `conversation/` into standalone agencies (still per agent) so prosodic alignment and negotiation live behind clear APIs.
5. **Docs**
   - Chapters for Engines & Mapping (how animation agency talks to EngineThree/Babylon/VRoid/RPM) and Persona Systems (trait weights, activation).

## Workstream 3 – Multi-Agent / Multi-Human Coordination

1. **Adaptive Agency Graphs**
   - Support runtime toggles for each agent’s agencies (e.g., switch to “minimal gesture” profiles without muting speech). Provide policy presets (debate, interview).
2. **Memory & Knowledge**
   - Implement per-agent memory stores (vector + symbolic) with APIs that agencies call to fetch/commit context. No direct sharing; cross-agent info only flows via signals.
3. **Conflict & Negotiation Scripts**
   - Add programmable patterns inside `conversation/` (majority vote, weighted expertise) and ensure outputs remain curves/snippets/audio through the animation/TTS agencies.
4. **Simulation Sandbox**
   - Headless runner that instantiates N agents + M humans, replays mic/webcam data, and records emitted cues for debugging. Include LiveKit-fed mic streams.
5. **Docs**
   - Chapters for Backend Orchestration (Python + LiveKit), Planner & Memory, Director/Synthesizer.

## Workstream 4 – Tooling & Ecosystem

1. **Agency Graph IDE**
   - Visual editor for configuring per-agent agency bundles, persona traits, and signal wiring. Exports config consumed by the per-agent factory.
2. **Monitoring & Metrics**
   - Dashboard per agent showing mic levels, transcription latency, viseme accuracy, prosodic density, gaze stability, etc.
3. **Plugin Ecosystem**
   - SDK for creating new agencies or swapping implementations, with contract tests ensuring they accept mic/webcam inputs and emit animation curves/audio through the expected APIs.
4. **Avatar Adapter Enhancements**
   - Expand animation mappings so `animation/` can drive VRoid, Ready Player Me, and other rigs with reduced geometry; include retargeting presets.
5. **Docs**
   - Chapter on Tooling & Sandbox plus appendices tying agencies to the referenced literature (Minsky, Emotion Machine, Lugrin & Pelachaud, Gumperz, Oxford Handbook of Facial Perception, CBFT).

## Success Metrics

- Every statement in documentation references actual files/README sections.
- All agencies follow the trinity pattern (or explicitly justify why not).
- Per-agent factory proves multiple agents/humans can run concurrently with isolated state.
- LiveKit integration lets agencies swap between local and remote sensor sources without code changes.
- Animation agency remains the sole path for applying curves/snippets, with tests verifying other agencies only emit data.

This roadmap replaces the previous drafts and removes the mistaken “history large” wording. The focus stays on the real code: each agency folder, how it handles mic/webcam inputs, and how its outputs feed the host engine through the animation agency.
